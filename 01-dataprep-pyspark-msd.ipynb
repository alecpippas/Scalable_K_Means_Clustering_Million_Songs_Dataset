{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-12T11:43:37.853119Z",
     "iopub.status.busy": "2025-05-12T11:43:37.852796Z",
     "iopub.status.idle": "2025-05-12T11:43:39.443910Z",
     "shell.execute_reply": "2025-05-12T11:43:39.443026Z",
     "shell.execute_reply.started": "2025-05-12T11:43:37.853094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "### Notebook Initialization â€“ Objective & Imports\n",
    "\n",
    "# Objective\n",
    "# - Process the 10,000-song subset using PySpark.\n",
    "# - Aggregate time-series features (mean, max, min, std).\n",
    "# - Store the processed data in Parquet format.\n",
    "# - (Optional) Push data to MongoDB Atlas.\n",
    "\n",
    "# Import Libraries\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, min, max\n",
    "from pymongo import MongoClient\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:43:39.445906Z",
     "iopub.status.busy": "2025-05-12T11:43:39.445160Z",
     "iopub.status.idle": "2025-05-12T11:43:46.535675Z",
     "shell.execute_reply": "2025-05-12T11:43:46.534542Z",
     "shell.execute_reply.started": "2025-05-12T11:43:39.445884Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/12 11:43:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session reconfigured for CPU processing.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "# Stop existing Spark session if running\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Reconfigure Spark for CPU usage\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"24g\")  # Maximize memory usage for large data\n",
    "conf.set(\"spark.driver.memory\", \"24g\")\n",
    "conf.set(\"spark.executor.cores\", \"4\")  # Utilize all 6 CPU cores\n",
    "conf.set(\"spark.task.cpus\", \"1\")       # 1 CPU core per task\n",
    "\n",
    "# Reinitialize Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"MSD_DataPrep_CPU\").getOrCreate()\n",
    "\n",
    "print(\"Spark session reconfigured for CPU processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:43:46.537932Z",
     "iopub.status.busy": "2025-05-12T11:43:46.537655Z",
     "iopub.status.idle": "2025-05-12T11:43:46.543796Z",
     "shell.execute_reply": "2025-05-12T11:43:46.542929Z",
     "shell.execute_reply.started": "2025-05-12T11:43:46.537908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths defined successfully\n"
     ]
    }
   ],
   "source": [
    "### Path Definitions\n",
    "\n",
    "INPUT_PATH = \"/kaggle/input/millionsongsubset/MillionSongSubset/*/*/*/*.h5\"\n",
    "\n",
    "# MongoDB Connection String (Optional)\n",
    "MONGO_URI = \"mongodb+srv://admin:yourpassword123@bigdatahw.udemiib.mongodb.net/?retryWrites=true&w=majority&appName=BigDataHW\"\n",
    "\n",
    "print(\"Paths defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:43:46.545087Z",
     "iopub.status.busy": "2025-05-12T11:43:46.544729Z",
     "iopub.status.idle": "2025-05-12T11:43:47.986306Z",
     "shell.execute_reply": "2025-05-12T11:43:47.985341Z",
     "shell.execute_reply.started": "2025-05-12T11:43:46.545047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB connection and sample insertion successful\n"
     ]
    }
   ],
   "source": [
    "### MongoDB Connection (Optional)\n",
    "\n",
    "# Test MongoDB Connection\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[\"msd_database\"]\n",
    "    collection = db[\"test_collection\"]\n",
    "\n",
    "    # Insert sample document\n",
    "    sample_doc = {\"status\": \"Connection successful\"}\n",
    "    collection.insert_one(sample_doc)\n",
    "\n",
    "    print(\"MongoDB connection and sample insertion successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"MongoDB connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:43:48.562961Z",
     "iopub.status.busy": "2025-05-12T11:43:48.562623Z",
     "iopub.status.idle": "2025-05-12T11:43:54.526822Z",
     "shell.execute_reply": "2025-05-12T11:43:54.525824Z",
     "shell.execute_reply.started": "2025-05-12T11:43:48.562936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis: <class 'h5py._hl.group.Group'>\n",
      "analysis/bars_confidence: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/bars_start: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/beats_confidence: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/beats_start: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/sections_confidence: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/sections_start: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/segments_confidence: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/segments_loudness_max: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/segments_loudness_max_time: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/segments_loudness_start: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/segments_pitches: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/segments_start: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/segments_timbre: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/songs: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/tatums_confidence: <class 'h5py._hl.dataset.Dataset'>\n",
      "analysis/tatums_start: <class 'h5py._hl.dataset.Dataset'>\n",
      "metadata: <class 'h5py._hl.group.Group'>\n",
      "metadata/artist_terms: <class 'h5py._hl.dataset.Dataset'>\n",
      "metadata/artist_terms_freq: <class 'h5py._hl.dataset.Dataset'>\n",
      "metadata/artist_terms_weight: <class 'h5py._hl.dataset.Dataset'>\n",
      "metadata/similar_artists: <class 'h5py._hl.dataset.Dataset'>\n",
      "metadata/songs: <class 'h5py._hl.dataset.Dataset'>\n",
      "musicbrainz: <class 'h5py._hl.group.Group'>\n",
      "musicbrainz/artist_mbtags: <class 'h5py._hl.dataset.Dataset'>\n",
      "musicbrainz/artist_mbtags_count: <class 'h5py._hl.dataset.Dataset'>\n",
      "musicbrainz/songs: <class 'h5py._hl.dataset.Dataset'>\n",
      "Sample HDF5 file structure displayed successfully\n"
     ]
    }
   ],
   "source": [
    "### Data Loading and Exploration - Inspecting HDF5 Structure\n",
    "\n",
    "# Select a sample file to inspect\n",
    "sample_file = glob.glob(INPUT_PATH)[0]\n",
    "\n",
    "# Open the file and display its structure\n",
    "with h5py.File(sample_file, 'r') as hdf:\n",
    "    def print_structure(name, obj):\n",
    "        print(f\"{name}: {type(obj)}\")\n",
    "    \n",
    "    hdf.visititems(print_structure)\n",
    "\n",
    "print(\"Sample HDF5 file structure displayed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:43:54.528737Z",
     "iopub.status.busy": "2025-05-12T11:43:54.528382Z",
     "iopub.status.idle": "2025-05-12T11:43:54.540959Z",
     "shell.execute_reply": "2025-05-12T11:43:54.540039Z",
     "shell.execute_reply.started": "2025-05-12T11:43:54.528704Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction function updated to include artist name sanitization.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata #part of standard library providing access to the Unicode Character Database (UCD)\n",
    "import re\n",
    "\n",
    "def sanitize_artist_name(artist_name):\n",
    "    \"\"\"\n",
    "    Sanitize artist name:\n",
    "    - Apply Unicode normalization (NFKD) to remove accents.\n",
    "    - Convert to lowercase.\n",
    "    - Remove special characters.\n",
    "    - Truncate to 30 characters.\n",
    "    \"\"\"\n",
    "    # Normalize to remove accents\n",
    "    sanitized_name = unicodedata.normalize('NFKD', artist_name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    sanitized_name = sanitized_name.lower()\n",
    "    \n",
    "    # Remove special characters and limit length to 30 characters\n",
    "    sanitized_name = re.sub(r'[^\\w\\s]', '', sanitized_name).strip()[:30]\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    sanitized_name = sanitized_name.replace(\" \", \"_\")\n",
    "\n",
    "    # Handle empty or missing artist names\n",
    "    if not sanitized_name:\n",
    "        sanitized_name = \"unknown_artist\"\n",
    "\n",
    "    return sanitized_name\n",
    "\n",
    "# Enhanced Extraction Function with Sanitization\n",
    "def extract_data_with_normalization(file_path):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as hdf:\n",
    "            # Extract metadata\n",
    "            try:\n",
    "                song_id = hdf['metadata/songs']['song_id'][0].decode('utf-8')\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                artist_name = hdf['metadata/songs']['artist_name'][0].decode('utf-8')\n",
    "                artist_name = sanitize_artist_name(artist_name)\n",
    "            except:\n",
    "                artist_name = \"unknown_artist\"\n",
    "\n",
    "            try:\n",
    "                title = hdf['metadata/songs']['title'][0].decode('utf-8')\n",
    "            except:\n",
    "                title = \"unknown_title\"\n",
    "\n",
    "            # Extract and aggregate time-series data\n",
    "            try:\n",
    "                timbre = hdf['analysis/segments_timbre'][:]\n",
    "                pitch = hdf['analysis/segments_pitches'][:]\n",
    "\n",
    "                # Skip if data is missing\n",
    "                if timbre.size == 0 or pitch.size == 0:\n",
    "                    return None\n",
    "\n",
    "                # Compute statistics\n",
    "                t_mean = np.mean(timbre, axis=0).tolist()\n",
    "                t_max = np.max(timbre, axis=0).tolist()\n",
    "                t_min = np.min(timbre, axis=0).tolist()\n",
    "                t_std = np.std(timbre, axis=0).tolist()\n",
    "\n",
    "                p_mean = np.mean(pitch, axis=0).tolist()\n",
    "                p_max = np.max(pitch, axis=0).tolist()\n",
    "                p_min = np.min(pitch, axis=0).tolist()\n",
    "                p_std = np.std(pitch, axis=0).tolist()\n",
    "\n",
    "                return Row(\n",
    "                    song_id=song_id,\n",
    "                    artist_name=artist_name,\n",
    "                    title=title,\n",
    "                    timbre_mean=t_mean,\n",
    "                    timbre_max=t_max,\n",
    "                    timbre_min=t_min,\n",
    "                    timbre_std=t_std,\n",
    "                    pitch_mean=p_mean,\n",
    "                    pitch_max=p_max,\n",
    "                    pitch_min=p_min,\n",
    "                    pitch_std=p_std\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Data extraction error: {e}\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"File processing error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Extraction function updated to include artist name sanitization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:43:56.224512Z",
     "iopub.status.busy": "2025-05-12T11:43:56.224182Z",
     "iopub.status.idle": "2025-05-12T11:43:56.230614Z",
     "shell.execute_reply": "2025-05-12T11:43:56.229704Z",
     "shell.execute_reply.started": "2025-05-12T11:43:56.224456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType\n",
    "\n",
    "# Define Schema Without 'year'\n",
    "schema_by_artist = StructType([\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"timbre_mean\", ArrayType(FloatType()), True),\n",
    "    StructField(\"timbre_max\", ArrayType(FloatType()), True),\n",
    "    StructField(\"timbre_min\", ArrayType(FloatType()), True),\n",
    "    StructField(\"timbre_std\", ArrayType(FloatType()), True),\n",
    "    StructField(\"pitch_mean\", ArrayType(FloatType()), True),\n",
    "    StructField(\"pitch_max\", ArrayType(FloatType()), True),\n",
    "    StructField(\"pitch_min\", ArrayType(FloatType()), True),\n",
    "    StructField(\"pitch_std\", ArrayType(FloatType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:43:58.488795Z",
     "iopub.status.busy": "2025-05-12T11:43:58.488423Z",
     "iopub.status.idle": "2025-05-12T11:44:10.588093Z",
     "shell.execute_reply": "2025-05-12T11:44:10.587312Z",
     "shell.execute_reply.started": "2025-05-12T11:43:58.488766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- timbre_mean: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- timbre_max: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- timbre_min: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- timbre_std: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- pitch_mean: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- pitch_max: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- pitch_min: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- pitch_std: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+-------------------+-------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|song_id           |artist_name   |title              |timbre_mean                                                                                                                                |timbre_max                                                                                            |timbre_min                                                                                                      |timbre_std                                                                                                                         |pitch_mean                                                                                                                                    |pitch_max                                                     |pitch_min                                                                           |pitch_std                                                                                                                                     |\n",
      "+------------------+--------------+-------------------+-------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|SOTEDAD12A8AE4735F|orbital       |Forever            |[39.32157, 6.516844, 23.708578, 10.636436, 1.148572, -17.32453, 12.015962, 6.970745, 2.7694502, 6.7983475, 5.452519, 11.377993]            |[48.838, 191.813, 189.508, 142.044, 105.988, 105.672, 112.282, 134.855, 131.468, 93.37, 73.64, 92.865]|[0.0, -248.26, -173.738, -86.369, -87.855, -101.944, -103.112, -74.671, -72.988, -57.473, -59.106, -76.753]     |[5.5113497, 69.0174, 55.502148, 32.18201, 25.470156, 26.33698, 29.416636, 19.29666, 22.770086, 19.401932, 14.850522, 25.389172]    |[0.41009885, 0.3645566, 0.4681166, 0.24636468, 0.351464, 0.34664443, 0.3190288, 0.4943671, 0.3735043, 0.6199981, 0.27316123, 0.20010029]      |[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  |[0.002, 0.004, 0.003, 0.003, 0.007, 0.004, 0.01, 0.01, 0.01, 0.024, 0.01, 0.001]    |[0.29420403, 0.3058539, 0.33243927, 0.23694986, 0.2863948, 0.28194582, 0.27449688, 0.33208942, 0.33098483, 0.34342957, 0.26544106, 0.23637629]|\n",
      "|SOUWZWV12AB0181ACF|kris_gruen    |Prayer Walk        |[43.890278, -89.25192, -22.637129, -22.975597, -23.777142, -14.043555, -14.108288, -12.219806, 7.1182775, -5.667286, 4.060265, 14.968142]  |[50.505, 171.13, 139.677, 334.288, 73.984, 117.895, 42.617, 151.994, 59.642, 83.307, 59.865, 72.0]    |[0.0, -248.416, -136.031, -137.85, -111.686, -209.599, -90.968, -81.224, -49.713, -91.435, -95.432, -92.687]    |[4.5190134, 49.744537, 33.571102, 33.39628, 30.702156, 23.616903, 23.876318, 22.826027, 17.3169, 17.42368, 14.414447, 18.152456]   |[0.373286, 0.11693737, 0.3490146, 0.3452881, 0.14585803, 0.256405, 0.12139666, 0.35558665, 0.10504593, 0.2702693, 0.29261377, 0.2315073]      |[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.976, 1.0, 1.0, 1.0]|[0.003, 0.006, 0.005, 0.003, 0.007, 0.005, 0.004, 0.007, 0.006, 0.007, 0.014, 0.006]|[0.32932666, 0.16484505, 0.3315956, 0.38314202, 0.1947592, 0.31272036, 0.13240612, 0.34252578, 0.113607764, 0.30697826, 0.31613627, 0.3132086]|\n",
      "|SOFAJOM12A8C141EE4|4_skins       |New War            |[46.810226, 41.306023, 40.84329, -3.4699106, 1.4934152, -15.112153, -7.938532, -0.6474308, 0.13559249, -6.3443274, 0.19135575, -5.984313]  |[52.869, 171.13, 127.591, 45.838, 70.959, 129.379, 27.987, 67.904, 38.288, 75.735, 56.385, 56.79]     |[0.0, -131.08, -65.464, -65.676, -93.561, -63.982, -58.102, -70.272, -37.499, -72.956, -26.657, -35.918]        |[5.7126975, 39.414158, 35.399014, 17.441784, 21.887815, 15.147407, 12.880156, 13.300988, 12.096301, 12.087529, 8.880887, 11.631738]|[0.49168435, 0.4256753, 0.57448125, 0.46158475, 0.5579133, 0.44376713, 0.4069586, 0.37351745, 0.323696, 0.3698331, 0.31854594, 0.48803493]    |[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  |[0.009, 0.016, 0.026, 0.027, 0.022, 0.009, 0.014, 0.017, 0.01, 0.012, 0.008, 0.008] |[0.3070258, 0.21079274, 0.30251172, 0.23553446, 0.31178215, 0.3031297, 0.22734675, 0.25663072, 0.19446658, 0.22494668, 0.21686706, 0.2917941] |\n",
      "|SOCCBOH12A8C13E947|rockit        |Some Kind of Record|[46.884056, 31.062284, 12.591344, 4.440421, -24.193872, -17.151476, 1.594063, -0.14998066, 4.386417, 3.472877, 0.4062523, -9.535078]       |[54.18, 129.396, 105.949, 403.623, 58.634, 121.904, 79.383, 82.986, 59.184, 90.421, 74.954, 40.155]   |[25.819, -100.153, -126.561, -105.848, -135.016, -220.891, -66.559, -67.587, -75.609, -74.369, -46.294, -55.048]|[5.046649, 45.231953, 30.739801, 34.359295, 31.300114, 29.519684, 20.41854, 17.578943, 17.08293, 13.755871, 14.2369795, 13.266249] |[0.38033366, 0.4429583, 0.3209461, 0.32677314, 0.5802635, 0.34298676, 0.34600103, 0.44966125, 0.30845678, 0.37428585, 0.2808647, 0.43238047]  |[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  |[0.014, 0.022, 0.016, 0.016, 0.021, 0.022, 0.025, 0.034, 0.03, 0.015, 0.015, 0.017] |[0.2728266, 0.32914257, 0.26057515, 0.21983574, 0.35199875, 0.24459752, 0.22776267, 0.3060521, 0.20539764, 0.31893018, 0.21329057, 0.31681713]|\n",
      "|SOBBWJS12AB0182522|betika__daouda|C'est pas ma faute |[44.009483, -0.6849293, -35.77042, -14.735756, 6.2774663, -14.848255, -26.523798, -4.3030953, -16.662634, -0.21954964, 4.328041, 6.9993715]|[52.167, 171.13, 72.251, 358.989, 117.217, 123.232, 62.874, 117.259, 54.2, 65.083, 70.645, 83.057]    |[0.0, -176.914, -147.173, -149.252, -86.484, -92.062, -111.382, -86.539, -79.771, -67.689, -63.071, -58.653]    |[4.067944, 40.635242, 32.472378, 40.18538, 31.284431, 29.280909, 25.025349, 23.982462, 19.55139, 15.617327, 18.976736, 18.859598]  |[0.44603336, 0.28118825, 0.32967433, 0.23954408, 0.20299524, 0.44371328, 0.20252979, 0.39092216, 0.2084456, 0.35885623, 0.3881549, 0.21603574]|[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  |[0.005, 0.005, 0.003, 0.003, 0.003, 0.004, 0.004, 0.002, 0.004, 0.005, 0.005, 0.006]|[0.32965237, 0.23357983, 0.28927004, 0.27708104, 0.19480968, 0.36366886, 0.17878048, 0.35195893, 0.19952497, 0.3063006, 0.33185014, 0.2212424]|\n",
      "+------------------+--------------+-------------------+-------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate DataFrame using the updated extraction function\n",
    "files = glob.glob(INPUT_PATH, recursive=True)\n",
    "\n",
    "data_rdd = spark.sparkContext.parallelize(files, numSlices=6) \\\n",
    "             .map(extract_data_with_normalization) \\\n",
    "             .filter(lambda x: x is not None)\n",
    "\n",
    "# Create DataFrame\n",
    "spark_df = spark.createDataFrame(data_rdd, schema_by_artist)\n",
    "\n",
    "# Verify schema and preview data\n",
    "spark_df.printSchema()\n",
    "spark_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:44:10.589555Z",
     "iopub.status.busy": "2025-05-12T11:44:10.589246Z",
     "iopub.status.idle": "2025-05-12T11:46:14.588027Z",
     "shell.execute_reply": "2025-05-12T11:46:14.587066Z",
     "shell.execute_reply.started": "2025-05-12T11:44:10.589528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to /kaggle/working/msd_agg_parquet_by_artist, partitioned by sanitized 'artist_name'.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the output path for partitioned Parquet data\n",
    "PARTITIONED_OUTPUT_PATH = \"/kaggle/working/msd_agg_parquet_by_artist\"\n",
    "\n",
    "# Remove the directory if it already exists\n",
    "if os.path.exists(PARTITIONED_OUTPUT_PATH):\n",
    "    shutil.rmtree(PARTITIONED_OUTPUT_PATH)\n",
    "\n",
    "# Export the DataFrame partitioned by 'artist_name'\n",
    "spark_df.write.mode(\"overwrite\").partitionBy(\"artist_name\").parquet(PARTITIONED_OUTPUT_PATH)\n",
    "\n",
    "print(f\"Data successfully exported to {PARTITIONED_OUTPUT_PATH}, partitioned by sanitized 'artist_name'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:46:17.631428Z",
     "iopub.status.busy": "2025-05-12T11:46:17.630614Z",
     "iopub.status.idle": "2025-05-12T11:46:40.319590Z",
     "shell.execute_reply": "2025-05-12T11:46:40.318801Z",
     "shell.execute_reply.started": "2025-05-12T11:46:17.631397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data exported to /kaggle/working/msd_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Define CSV output path\n",
    "CSV_OUTPUT_PATH = \"/kaggle/working/msd_sample.csv\"\n",
    "\n",
    "# Extract a sample of 1000 rows\n",
    "sample_df = spark_df.limit(1000)\n",
    "\n",
    "# Export the sample to CSV\n",
    "sample_df.toPandas().to_csv(CSV_OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Sample data exported to {CSV_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:47:11.798855Z",
     "iopub.status.busy": "2025-05-12T11:47:11.798561Z",
     "iopub.status.idle": "2025-05-12T11:47:11.850840Z",
     "shell.execute_reply": "2025-05-12T11:47:11.849761Z",
     "shell.execute_reply.started": "2025-05-12T11:47:11.798836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              song_id     artist_name                title  \\\n",
      "0  SOTEDAD12A8AE4735F         orbital              Forever   \n",
      "1  SOUWZWV12AB0181ACF      kris_gruen          Prayer Walk   \n",
      "2  SOFAJOM12A8C141EE4         4_skins              New War   \n",
      "3  SOCCBOH12A8C13E947          rockit  Some Kind of Record   \n",
      "4  SOBBWJS12AB0182522  betika__daouda   C'est pas ma faute   \n",
      "\n",
      "                                         timbre_mean  \\\n",
      "0  [39.321571350097656, 6.516843795776367, 23.708...   \n",
      "1  [43.89027786254883, -89.25192260742188, -22.63...   \n",
      "2  [46.81022644042969, 41.30602264404297, 40.8432...   \n",
      "3  [46.884056091308594, 31.062284469604492, 12.59...   \n",
      "4  [44.009483337402344, -0.6849293112754822, -35....   \n",
      "\n",
      "                                          timbre_max  \\\n",
      "0  [48.8380012512207, 191.81300354003906, 189.507...   \n",
      "1  [50.505001068115234, 171.1300048828125, 139.67...   \n",
      "2  [52.86899948120117, 171.1300048828125, 127.591...   \n",
      "3  [54.18000030517578, 129.39599609375, 105.94899...   \n",
      "4  [52.16699981689453, 171.1300048828125, 72.2509...   \n",
      "\n",
      "                                          timbre_min  \\\n",
      "0  [0.0, -248.25999450683594, -173.73800659179688...   \n",
      "1  [0.0, -248.41600036621094, -136.031005859375, ...   \n",
      "2  [0.0, -131.0800018310547, -65.46399688720703, ...   \n",
      "3  [25.819000244140625, -100.15299987792969, -126...   \n",
      "4  [0.0, -176.91400146484375, -147.17300415039062...   \n",
      "\n",
      "                                          timbre_std  \\\n",
      "0  [5.511349678039551, 69.01740264892578, 55.5021...   \n",
      "1  [4.519013404846191, 49.744537353515625, 33.571...   \n",
      "2  [5.712697505950928, 39.41415786743164, 35.3990...   \n",
      "3  [5.046648979187012, 45.23195266723633, 30.7398...   \n",
      "4  [4.067944049835205, 40.6352424621582, 32.47237...   \n",
      "\n",
      "                                          pitch_mean  \\\n",
      "0  [0.4100988507270813, 0.36455661058425903, 0.46...   \n",
      "1  [0.37328600883483887, 0.11693736910820007, 0.3...   \n",
      "2  [0.49168434739112854, 0.42567530274391174, 0.5...   \n",
      "3  [0.38033366203308105, 0.4429582953453064, 0.32...   \n",
      "4  [0.4460333585739136, 0.2811882495880127, 0.329...   \n",
      "\n",
      "                                           pitch_max  \\\n",
      "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
      "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.976...   \n",
      "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
      "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
      "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
      "\n",
      "                                           pitch_min  \\\n",
      "0  [0.0020000000949949026, 0.004000000189989805, ...   \n",
      "1  [0.003000000026077032, 0.006000000052154064, 0...   \n",
      "2  [0.008999999612569809, 0.01600000075995922, 0....   \n",
      "3  [0.014000000432133675, 0.02199999988079071, 0....   \n",
      "4  [0.004999999888241291, 0.004999999888241291, 0...   \n",
      "\n",
      "                                           pitch_std  \n",
      "0  [0.2942040264606476, 0.3058539032936096, 0.332...  \n",
      "1  [0.32932665944099426, 0.1648450493812561, 0.33...  \n",
      "2  [0.3070257902145386, 0.21079273521900177, 0.30...  \n",
      "3  [0.272826611995697, 0.32914257049560547, 0.260...  \n",
      "4  [0.3296523690223694, 0.2335798293352127, 0.289...  \n"
     ]
    }
   ],
   "source": [
    "# Read and display the first few rows\n",
    "sample_data = pd.read_csv(CSV_OUTPUT_PATH)\n",
    "print(sample_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:47:14.689591Z",
     "iopub.status.busy": "2025-05-12T11:47:14.689271Z",
     "iopub.status.idle": "2025-05-12T11:47:14.697234Z",
     "shell.execute_reply": "2025-05-12T11:47:14.695932Z",
     "shell.execute_reply.started": "2025-05-12T11:47:14.689569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema logged successfully at /kaggle/working/schema_log.txt\n"
     ]
    }
   ],
   "source": [
    "# Define schema log path\n",
    "SCHEMA_LOG_PATH = \"/kaggle/working/schema_log.txt\"\n",
    "\n",
    "# Log the schema to a text file\n",
    "with open(SCHEMA_LOG_PATH, \"w\") as file:\n",
    "    file.write(str(spark_df.schema))\n",
    "\n",
    "print(f\"Schema logged successfully at {SCHEMA_LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:47:17.370531Z",
     "iopub.status.busy": "2025-05-12T11:47:17.370189Z",
     "iopub.status.idle": "2025-05-12T11:47:17.376523Z",
     "shell.execute_reply": "2025-05-12T11:47:17.375495Z",
     "shell.execute_reply.started": "2025-05-12T11:47:17.370505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('song_id', StringType(), True), StructField('artist_name', StringType(), True), StructField('title', StringType(), True), StructField('timbre_mean', ArrayType(FloatType(), True), True), StructField('timbre_max', ArrayType(FloatType(), True), True), StructField('timbre_min', ArrayType(FloatType(), True), True), StructField('timbre_std', ArrayType(FloatType(), True), True), StructField('pitch_mean', ArrayType(FloatType(), True), True), StructField('pitch_max', ArrayType(FloatType(), True), True), StructField('pitch_min', ArrayType(FloatType(), True), True), StructField('pitch_std', ArrayType(FloatType(), True), True)])\n"
     ]
    }
   ],
   "source": [
    "# Display the first few lines of the schema log\n",
    "with open(SCHEMA_LOG_PATH, \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:47:20.058609Z",
     "iopub.status.busy": "2025-05-12T11:47:20.057720Z",
     "iopub.status.idle": "2025-05-12T11:47:20.340968Z",
     "shell.execute_reply": "2025-05-12T11:47:20.340020Z",
     "shell.execute_reply.started": "2025-05-12T11:47:20.058576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet Size: 31.85 MB\n",
      "CSV Sample Size: 1.75 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Calculate the total size of the directory in MB.\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Calculate Parquet and CSV sizes\n",
    "parquet_size = get_dir_size(PARTITIONED_OUTPUT_PATH)\n",
    "csv_size = os.path.getsize(CSV_OUTPUT_PATH) / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "print(f\"Parquet Size: {parquet_size:.2f} MB\")\n",
    "print(f\"CSV Sample Size: {csv_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:48:26.124391Z",
     "iopub.status.busy": "2025-05-12T11:48:26.124112Z",
     "iopub.status.idle": "2025-05-12T11:48:29.711928Z",
     "shell.execute_reply": "2025-05-12T11:48:29.711190Z",
     "shell.execute_reply.started": "2025-05-12T11:48:26.124371Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping completed. ZIP file created at: /kaggle/working/msd_parquet_by_artist.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define ZIP output path\n",
    "ZIP_OUTPUT_PATH = \"/kaggle/working/msd_parquet_by_artist.zip\"\n",
    "\n",
    "# Path to Parquet directory\n",
    "PARQUET_DIR = \"/kaggle/working/msd_agg_parquet_by_artist\"\n",
    "\n",
    "# Create ZIP\n",
    "shutil.make_archive(ZIP_OUTPUT_PATH.replace(\".zip\", \"\"), 'zip', PARQUET_DIR)\n",
    "\n",
    "print(f\"Zipping completed. ZIP file created at: {ZIP_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:48:35.994133Z",
     "iopub.status.busy": "2025-05-12T11:48:35.993850Z",
     "iopub.status.idle": "2025-05-12T11:48:36.447947Z",
     "shell.execute_reply": "2025-05-12T11:48:36.447235Z",
     "shell.execute_reply.started": "2025-05-12T11:48:35.994112Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing MongoDB collection 'song_data' dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Drop existing collection\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[\"msd_database\"]\n",
    "    db[\"song_data\"].drop()\n",
    "    print(\"Existing MongoDB collection 'song_data' dropped successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error dropping MongoDB collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:48:40.156534Z",
     "iopub.status.busy": "2025-05-12T11:48:40.156238Z",
     "iopub.status.idle": "2025-05-12T11:50:21.773542Z",
     "shell.execute_reply": "2025-05-12T11:50:21.772439Z",
     "shell.execute_reply.started": "2025-05-12T11:48:40.156512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB connection established successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1 of 11\n",
      "Inserted batch 2 of 11\n",
      "Inserted batch 3 of 11\n",
      "Inserted batch 4 of 11\n",
      "Inserted batch 5 of 11\n",
      "Inserted batch 6 of 11\n",
      "Inserted batch 7 of 11\n",
      "Inserted batch 8 of 11\n",
      "Inserted batch 9 of 11\n",
      "Inserted batch 10 of 11\n",
      "Data re-ingested to MongoDB successfully.\n"
     ]
    }
   ],
   "source": [
    "# MongoDB Ingestion - Using Defined Path and Connection\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Establish MongoDB Connection\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[\"msd_database\"]\n",
    "    collection = db[\"song_data\"]\n",
    "    print(\"MongoDB connection established successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"MongoDB connection failed: {e}\")\n",
    "\n",
    "# Function to Convert DataFrame Row to Dictionary\n",
    "# Function to convert DataFrame rows to MongoDB-compatible dictionaries\n",
    "def row_to_dict(row):\n",
    "    return {\n",
    "        \"song_id\": row.song_id,\n",
    "        \"artist_name\": row.artist_name,\n",
    "        \"title\": row.title,\n",
    "        \"timbre_mean\": row.timbre_mean,\n",
    "        \"timbre_max\": row.timbre_max,\n",
    "        \"timbre_min\": row.timbre_min,\n",
    "        \"timbre_std\": row.timbre_std,\n",
    "        \"pitch_mean\": row.pitch_mean,\n",
    "        \"pitch_max\": row.pitch_max,\n",
    "        \"pitch_min\": row.pitch_min,\n",
    "        \"pitch_std\": row.pitch_std\n",
    "    }\n",
    "\n",
    "# Re-ingest data to MongoDB\n",
    "BATCH_SIZE = 1000\n",
    "data = spark_df.rdd.map(row_to_dict).collect()\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(data), BATCH_SIZE):\n",
    "        batch = data[i:i + BATCH_SIZE]\n",
    "        collection.insert_many(batch, ordered=False)\n",
    "        print(f\"Inserted batch {i // BATCH_SIZE + 1} of {len(data) // BATCH_SIZE + 1}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during MongoDB ingestion: {e}\")\n",
    "\n",
    "print(\"Data re-ingested to MongoDB successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:50:28.504389Z",
     "iopub.status.busy": "2025-05-12T11:50:28.504117Z",
     "iopub.status.idle": "2025-05-12T11:50:28.861876Z",
     "shell.execute_reply": "2025-05-12T11:50:28.860861Z",
     "shell.execute_reply.started": "2025-05-12T11:50:28.504370Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created on 'song_id' (Unique).\n",
      "Index created on 'artist_name'.\n",
      "Index created on 'title'.\n"
     ]
    }
   ],
   "source": [
    "# Ensure MongoDB connection is still active\n",
    "try:\n",
    "    # Create Unique Index on song_id\n",
    "    collection.create_index(\"song_id\", unique=True)\n",
    "    print(\"Index created on 'song_id' (Unique).\")\n",
    "\n",
    "    # Create Index on artist_name\n",
    "    collection.create_index(\"artist_name\")\n",
    "    print(\"Index created on 'artist_name'.\")\n",
    "\n",
    "    # Create Index on title\n",
    "    collection.create_index(\"title\")\n",
    "    print(\"Index created on 'title'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Index creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7392615,
     "sourceId": 11782323,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
